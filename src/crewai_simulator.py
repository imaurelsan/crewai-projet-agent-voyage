"""
Simulateur CrewAI utilisant LangChain + Ollama
Imite l'architecture CrewAI pour compatibilit√© avec les exemples de cours
"""

import os
import yaml
from typing import List, Dict, Any, Callable
from functools import wraps
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

load_dotenv()

# ============= Configuration =============
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.2:3b")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")


# ============= Classes de Base =============

class Agent:
    """Simule un agent CrewAI"""
    
    def __init__(self, role: str, goal: str, backstory: str, 
                 verbose: bool = True, tools: List = None, llm: Any = None):
        self.role = role
        self.goal = goal
        self.backstory = backstory
        self.verbose = verbose
        self.tools = tools or []
        self.llm = llm
        
    def __repr__(self):
        return f"Agent(role='{self.role}')"


class Task:
    """Simule une t√¢che CrewAI"""
    
    def __init__(self, description: str, expected_output: str, 
                 agent: Agent = None, context: List = None, output_file: str = None):
        self.description = description
        self.expected_output = expected_output
        self.agent = agent
        self.context = context or []
        self.output_file = output_file
        
    def __repr__(self):
        return f"Task(agent={self.agent.role if self.agent else 'None'})"


class Crew:
    """Simule une √©quipe CrewAI"""
    
    def __init__(self, agents: List[Agent], tasks: List[Task], 
                 verbose: bool = True, process: str = "sequential"):
        self.agents = agents
        self.tasks = tasks
        self.verbose = verbose
        self.process = process
        self.llm = OllamaLLM(model=OLLAMA_MODEL, base_url=OLLAMA_BASE_URL, temperature=0.7)
        
    def kickoff(self, inputs: Dict[str, Any] = None) -> str:
        """Lance l'ex√©cution de l'√©quipe"""
        inputs = inputs or {}
        
        if self.verbose:
            print("=" * 70)
            print("üöÄ D√âMARRAGE DE L'√âQUIPE")
            print("=" * 70)
            print(f"üìã Agents: {len(self.agents)}")
            print(f"üìù T√¢ches: {len(self.tasks)}")
            print(f"‚öôÔ∏è  Processus: {self.process}")
            print(f"ü§ñ LLM: {OLLAMA_MODEL}")
            print("=" * 70)
            print()
        
        results = []
        context_history = ""
        
        for i, task in enumerate(self.tasks, 1):
            if self.verbose:
                print(f"\n{'='*70}")
                print(f"üìå T√ÇCHE {i}/{len(self.tasks)}: {task.agent.role if task.agent else 'Agent non assign√©'}")
                print(f"{'='*70}")
            
            # Remplacer les variables dans la description
            description = task.description
            for key, value in inputs.items():
                description = description.replace(f"{{{key}}}", str(value))
            
            # Ajouter le contexte des t√¢ches pr√©c√©dentes si sp√©cifi√©
            full_context = context_history
            if task.context:
                for j, context_task in enumerate(task.context):
                    # Utiliser l'index de la boucle au lieu de chercher dans la liste
                    if j < len(results):
                        agent_role = context_task.agent.role if context_task.agent else "Agent"
                        full_context += f"\n\nContexte de '{agent_role}':\n{results[j]}"
            
            # Cr√©er le prompt
            if full_context:
                prompt_text = f"""Tu es {task.agent.role}.

Ton objectif: {task.agent.goal}

Background: {task.agent.backstory}

Contexte des t√¢ches pr√©c√©dentes:
{full_context}

T√¢che √† accomplir:
{description}

Output attendu: {task.expected_output}

R√©ponds de mani√®re d√©taill√©e et professionnelle."""
            else:
                prompt_text = f"""Tu es {task.agent.role}.

Ton objectif: {task.agent.goal}

Background: {task.agent.backstory}

T√¢che √† accomplir:
{description}

Output attendu: {task.expected_output}

R√©ponds de mani√®re d√©taill√©e et professionnelle."""
            
            # Ex√©cuter avec le LLM
            if self.verbose:
                print(f"ü§ñ {task.agent.role} travaille...\n")
            
            # Utiliser les outils si disponibles
            if task.agent.tools:
                # Appeler les outils si n√©cessaire
                tool_results = []
                for tool in task.agent.tools:
                    if callable(tool):
                        try:
                            # Extraire les param√®tres depuis les inputs
                            tool_result = tool.invoke(inputs)
                            tool_results.append(tool_result)
                        except Exception as e:
                            if self.verbose:
                                print(f"‚ö†Ô∏è  Erreur outil: {e}")
                
                if tool_results:
                    prompt_text += f"\n\nR√©sultats des outils:\n" + "\n".join(tool_results)
            
            result = self.llm.invoke(prompt_text)
            results.append(result)
            context_history += f"\n\n=== {task.agent.role} ===\n{result}"
            
            if self.verbose:
                print(f"‚úÖ R√©sultat:\n{result}")
            
            # Sauvegarder dans un fichier si sp√©cifi√©
            if task.output_file:
                with open(task.output_file, 'w', encoding='utf-8') as f:
                    f.write(result)
                if self.verbose:
                    print(f"üíæ Sauvegard√© dans: {task.output_file}")
        
        if self.verbose:
            print(f"\n{'='*70}")
            print("‚ú® √âQUIPE TERMIN√âE")
            print(f"{'='*70}\n")
        
        # Retourner le dernier r√©sultat (g√©n√©ralement la synth√®se finale)
        return results[-1] if results else ""


# ============= D√©corateurs =============

def CrewBase(cls):
    """D√©corateur de classe pour simuler @CrewBase de CrewAI"""
    
    # Charger les configurations YAML
    config_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config')
    
    agents_config_path = os.path.join(config_dir, 'agents.yaml')
    tasks_config_path = os.path.join(config_dir, 'tasks.yaml')
    
    if os.path.exists(agents_config_path):
        with open(agents_config_path, 'r', encoding='utf-8') as f:
            cls.agents_config = yaml.safe_load(f)
    else:
        cls.agents_config = {}
    
    if os.path.exists(tasks_config_path):
        with open(tasks_config_path, 'r', encoding='utf-8') as f:
            cls.tasks_config = yaml.safe_load(f)
    else:
        cls.tasks_config = {}
    
    return cls


def agent(func: Callable) -> Callable:
    """D√©corateur pour marquer une m√©thode comme agent"""
    func._is_agent = True
    return func


def task(func: Callable) -> Callable:
    """D√©corateur pour marquer une m√©thode comme t√¢che"""
    func._is_task = True
    return func


def crew(func: Callable) -> Callable:
    """D√©corateur pour marquer une m√©thode comme crew"""
    func._is_crew = True
    return func


# ============= Process Enum =============

class Process:
    """Simule l'enum Process de CrewAI"""
    sequential = "sequential"
    hierarchical = "hierarchical"


# ============= LLM Helper =============

class LLM:
    """Wrapper pour le LLM compatible avec la syntaxe CrewAI"""
    
    def __init__(self, model: str = None, temperature: float = 0.7, base_url: str = None):
        self.model = model or OLLAMA_MODEL
        self.temperature = temperature
        self.base_url = base_url or OLLAMA_BASE_URL
        self._llm = OllamaLLM(
            model=self.model,
            base_url=self.base_url,
            temperature=self.temperature
        )
    
    def __repr__(self):
        return f"LLM(model='{self.model}')"
